{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa01f5a3-8349-45f5-b864-d25a217f5b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import random\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3bb10f-4834-4321-9128-984269d94a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"monologg/koelectra-base-v3-discriminator\"\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 5e-5\n",
    "\n",
    "print(f\"CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "111e14d2-e78b-46de-b699-c381c8d529a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_file = Path(\"dataset/TL/ìš©ë¡€_ê²Œì„tl.json\")\n",
    "val_input_file = Path(\"dataset/VL/ìš©ë¡€_ê²Œì„vl.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5894c96e-1129-4a42-8844-7ad13f9c8736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON íŒŒì¼ ë¡œë”© ì¤‘...\n",
      "ì´ 199,504ê°œ ìƒ˜í”Œ ë°œê²¬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training examples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 199504/199504 [00:02<00:00, 73609.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í•™ìŠµ ë°ì´í„°: 199,504ê°œ ì˜ˆì œ ì²˜ë¦¬ ì™„ë£Œ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_processed_data = []\n",
    "labels = set(['O'])\n",
    "\n",
    "print(\"JSON íŒŒì¼ ë¡œë”© ì¤‘...\")\n",
    "with open(train_input_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "total_samples = len(data)\n",
    "print(f\"ì´ {total_samples:,}ê°œ ìƒ˜í”Œ ë°œê²¬\")\n",
    "\n",
    "for example in tqdm(data, desc=\"Processing training examples\"):\n",
    "    sentence = example.get('sentence', '')\n",
    "    tokens = example.get('tokens', [])\n",
    "    \n",
    "    if sentence and tokens:\n",
    "        char_tags = ['O'] * len(sentence)\n",
    "        \n",
    "        for token in tokens:\n",
    "            start = token['start']\n",
    "            length = token['length']\n",
    "            facet = token.get('facet', 'TERM')\n",
    "            \n",
    "            if start < len(sentence):\n",
    "                char_tags[start] = f'B-{facet}'\n",
    "            \n",
    "            for i in range(start + 1, start + length):\n",
    "                if i < len(sentence):\n",
    "                    char_tags[i] = f'I-{facet}'\n",
    "        \n",
    "        chars = list(sentence)\n",
    "        tags = char_tags\n",
    "        labels.update(tags)\n",
    "        \n",
    "        train_processed_data.append({\n",
    "            'id': example.get('id'),\n",
    "            'sentence': sentence,\n",
    "            'chars': chars,\n",
    "            'tags': tags,\n",
    "            'tokens': tokens\n",
    "        })\n",
    "\n",
    "# ë©”ëª¨ë¦¬ ì •ë¦¬\n",
    "del data\n",
    "\n",
    "print(f\"í•™ìŠµ ë°ì´í„°: {len(train_processed_data):,}ê°œ ì˜ˆì œ ì²˜ë¦¬ ì™„ë£Œ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c7e5120-f274-4886-9beb-1e0f6fecc34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ê²€ì¦ ë°ì´í„° ì²˜ë¦¬ ì¤‘: dataset\\VL\\ìš©ë¡€_ê²Œì„vl.json\n",
      "JSON íŒŒì¼ ë¡œë”© ì¤‘...\n",
      "ì´ 24,938ê°œ ìƒ˜í”Œ ë°œê²¬\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing validation examples: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24938/24938 [00:00<00:00, 110602.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê²€ì¦ ë°ì´í„°: 24,938ê°œ ì˜ˆì œ ì²˜ë¦¬ ì™„ë£Œ\n",
      "\n",
      "ë°ì´í„° ë¶„í•  ì™„ë£Œ:\n",
      "  - Train: 179,553 samples\n",
      "  - Val: 24,938 samples\n",
      "  - Test: 19,951 samples\n"
     ]
    }
   ],
   "source": [
    "if val_input_file and val_input_file.exists():\n",
    "    print(f\"\\nê²€ì¦ ë°ì´í„° ì²˜ë¦¬ ì¤‘: {val_input_file}\")\n",
    "    val_processed_data = []\n",
    "    \n",
    "    print(\"JSON íŒŒì¼ ë¡œë”© ì¤‘...\")\n",
    "    with open(val_input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    total_samples = len(data)\n",
    "    print(f\"ì´ {total_samples:,}ê°œ ìƒ˜í”Œ ë°œê²¬\")\n",
    "    \n",
    "    for example in tqdm(data, desc=\"Processing validation examples\"):\n",
    "        sentence = example.get('sentence', '')\n",
    "        tokens = example.get('tokens', [])\n",
    "        \n",
    "        if sentence and tokens:\n",
    "            char_tags = ['O'] * len(sentence)\n",
    "            \n",
    "            for token in tokens:\n",
    "                start = token['start']\n",
    "                length = token['length']\n",
    "                facet = token.get('facet', 'TERM')\n",
    "                \n",
    "                if start < len(sentence):\n",
    "                    char_tags[start] = f'B-{facet}'\n",
    "                \n",
    "                for i in range(start + 1, start + length):\n",
    "                    if i < len(sentence):\n",
    "                        char_tags[i] = f'I-{facet}'\n",
    "            \n",
    "            chars = list(sentence)\n",
    "            tags = char_tags\n",
    "            labels.update(tags)\n",
    "            \n",
    "            val_processed_data.append({\n",
    "                'id': example.get('id'),\n",
    "                'sentence': sentence,\n",
    "                'chars': chars,\n",
    "                'tags': tags,\n",
    "                'tokens': tokens\n",
    "            })\n",
    "    \n",
    "    del data\n",
    "    \n",
    "    print(f\"ê²€ì¦ ë°ì´í„°: {len(val_processed_data):,}ê°œ ì˜ˆì œ ì²˜ë¦¬ ì™„ë£Œ\")\n",
    "    \n",
    "    random.shuffle(train_processed_data)\n",
    "    train_size = int(len(train_processed_data) * 0.9)\n",
    "    \n",
    "    train_data = train_processed_data[:train_size]\n",
    "    test_data = train_processed_data[train_size:]\n",
    "    val_data = val_processed_data\n",
    "    \n",
    "else:\n",
    "    print(\"\\nê²€ì¦ íŒŒì¼ì´ ì—†ì–´ í•™ìŠµ ë°ì´í„°ë¥¼ ë¶„í• í•©ë‹ˆë‹¤.\")\n",
    "    random.shuffle(train_processed_data)\n",
    "    total = len(train_processed_data)\n",
    "    train_size = int(total * 0.8)\n",
    "    val_size = int(total * 0.1)\n",
    "    \n",
    "    train_data = train_processed_data[:train_size]\n",
    "    val_data = train_processed_data[train_size:train_size + val_size]\n",
    "    test_data = train_processed_data[train_size + val_size:]\n",
    "\n",
    "print(f\"\\në°ì´í„° ë¶„í•  ì™„ë£Œ:\")\n",
    "print(f\"  - Train: {len(train_data):,} samples\")\n",
    "print(f\"  - Val: {len(val_data):,} samples\")\n",
    "print(f\"  - Test: {len(test_data):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cae24ce1-36c6-4a25-944f-1be75fa70bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì „ì²˜ë¦¬ ì™„ë£Œ! ë°ì´í„° ì €ì¥ë¨\n"
     ]
    }
   ],
   "source": [
    "with open(\"processed/train.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "with open(\"processed/val.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(val_data, f, ensure_ascii=False, indent=2)\n",
    "with open(\"processed/test.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(test_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "label2id = {label: idx for idx, label in enumerate(sorted(labels))}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "label_map = {\n",
    "    'label2id': label2id,\n",
    "    'id2label': id2label,\n",
    "    'num_labels': len(labels)\n",
    "}\n",
    "\n",
    "with open(\"processed/label_map.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(label_map, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"ì „ì²˜ë¦¬ ì™„ë£Œ! ë°ì´í„° ì €ì¥ë¨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cdbf4a7-922a-4b5e-8ea4-2baa5360e164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë ˆì´ë¸” ìˆ˜: 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "with open(\"processed/label_map.json\", 'r', encoding='utf-8') as f:\n",
    "    label_map = json.load(f)\n",
    "\n",
    "label2id = label_map['label2id']\n",
    "id2label = {int(k): v for k, v in label_map['id2label'].items()}\n",
    "num_labels = label_map['num_labels']\n",
    "\n",
    "print(f\"ë ˆì´ë¸” ìˆ˜: {num_labels}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    use_safetensors=True\n",
    ")\n",
    "\n",
    "print(\"ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "853c5c39-5ca0-4f73-b0ee-fbbf0d52338f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ë°ì´í„° ë¡œë”© ì¤‘...\n",
      "  - Train: 179553 samples\n",
      "  - Val: 24938 samples\n",
      "  - Test: 19951 samples\n",
      "\n",
      "í† í°í™” ì¤‘...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e03b92eec94f2f9924d791151b9960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/179553 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85f918d812e842749b21fd776cd3c0f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24938 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "í† í°í™” ì™„ë£Œ!\n"
     ]
    }
   ],
   "source": [
    "print(\"ë°ì´í„° ë¡œë”© ì¤‘...\")\n",
    "with open(\"processed/train.json\", 'r', encoding='utf-8') as f:\n",
    "    train_data = json.load(f)\n",
    "with open(\"processed/val.json\", 'r', encoding='utf-8') as f:\n",
    "    val_data = json.load(f)\n",
    "with open(\"processed/test.json\", 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"  - Train: {len(train_data)} samples\")\n",
    "print(f\"  - Val: {len(val_data)} samples\")\n",
    "print(f\"  - Test: {len(test_data)} samples\")\n",
    "\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "print(\"\\ní† í°í™” ì¤‘...\")\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['chars'],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label2id[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_train = train_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "tokenized_val = val_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names\n",
    ")\n",
    "\n",
    "print(\"í† í°í™” ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ec6ebd0-7f84-4734-ba0a-6234f3bded0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    true_predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        for pred_id, label_id in zip(prediction, label):\n",
    "            if label_id != -100:\n",
    "                true_predictions.append(pred_id)\n",
    "                true_labels.append(label_id)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, \n",
    "        true_predictions, \n",
    "        average='weighted',\n",
    "        zero_division=0\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18742cda-2ce7-4e6d-8ca6-d0ce0774cd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ëª¨ë¸ í•™ìŠµ ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhkst\\AppData\\Local\\Temp\\ipykernel_18452\\3999087097.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33669' max='33669' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33669/33669 2:34:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>0.324001</td>\n",
       "      <td>0.886310</td>\n",
       "      <td>0.896017</td>\n",
       "      <td>0.888616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>0.273065</td>\n",
       "      <td>0.902674</td>\n",
       "      <td>0.907787</td>\n",
       "      <td>0.903903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.204600</td>\n",
       "      <td>0.260626</td>\n",
       "      <td>0.908595</td>\n",
       "      <td>0.912654</td>\n",
       "      <td>0.910044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "í•™ìŠµ ì™„ë£Œ! ëª¨ë¸ ì €ì¥: models/final_model\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nëª¨ë¸ í•™ìŠµ ì‹œì‘...\")\n",
    "\n",
    "Path(\"models\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"models/final_model\")\n",
    "tokenizer.save_pretrained(\"models/final_model\")\n",
    "\n",
    "print(f\"\\ní•™ìŠµ ì™„ë£Œ! ëª¨ë¸ ì €ì¥: models/final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ba02620-f870-4762-b16b-8d0cdef627bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"models/final_model\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"models/final_model\",\n",
    "    use_safetensors=True\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Device: {device}\\n\")\n",
    "\n",
    "with open(\"processed/label_map.json\", 'r', encoding='utf-8') as f:\n",
    "    label_map = json.load(f)\n",
    "id2label = {int(k): v for k, v in label_map['id2label'].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8762b63-259f-482b-b1b8-ca5b0411b536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê²Œì„ ìš©ì–´ NER ì¶”ë¡ \n",
      "============================================================\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”:  íƒˆì§„ ë„ì ì´ ì•„ì§ë„ ë‚ ë›°ë‹¤ë‹ˆ..!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ì…ë ¥: íƒˆì§„ ë„ì ì´ ì•„ì§ë„ ë‚ ë›°ë‹¤ë‹ˆ..!\n",
      "ê²°ê³¼: 1ê°œ ìš©ì–´ ì¸ì‹\n",
      "  - íƒˆì§„ë„ì  (ì‹œìŠ¤í…œìš©ì–´, 52.50%)\n"
     ]
    }
   ],
   "source": [
    "print(\"ê²Œì„ ìš©ì–´ NER ì¶”ë¡ \")\n",
    "print(\"=\" * 60)\n",
    "sentence = input(\"ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”: \")\n",
    "\n",
    "if not sentence.strip():\n",
    "    print(\"ë¬¸ì¥ì´ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    inputs = tokenizer(sentence, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.argmax(outputs.logits, dim=2)\n",
    "        probabilities = torch.softmax(outputs.logits, dim=2)\n",
    "    \n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    predictions = predictions[0].cpu().numpy()\n",
    "    probabilities = probabilities[0].cpu().numpy()\n",
    "    \n",
    "    entities = []\n",
    "    current_entity = None\n",
    "    \n",
    "    for i, (token, pred_id) in enumerate(zip(tokens, predictions)):\n",
    "        if token in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "            continue\n",
    "        \n",
    "        label = id2label[pred_id]\n",
    "        confidence = probabilities[i][pred_id]\n",
    "        \n",
    "        if label.startswith('B-'):\n",
    "            if current_entity:\n",
    "                entities.append(current_entity)\n",
    "            \n",
    "            entity_type = label[2:]\n",
    "            current_entity = {\n",
    "                'term': token.replace('##', ''),\n",
    "                'label': entity_type,\n",
    "                'confidence': confidence\n",
    "            }\n",
    "        elif label.startswith('I-') and current_entity:\n",
    "            entity_type = label[2:]\n",
    "            if entity_type == current_entity['label']:\n",
    "                current_entity['term'] += token.replace('##', '')\n",
    "                current_entity['confidence'] = (current_entity['confidence'] + confidence) / 2\n",
    "        else:\n",
    "            if current_entity:\n",
    "                entities.append(current_entity)\n",
    "                current_entity = None\n",
    "    \n",
    "    if current_entity:\n",
    "        entities.append(current_entity)\n",
    "    \n",
    "    print(f\"\\nì…ë ¥: {sentence}\")\n",
    "    print(f\"ê²°ê³¼: {len(entities)}ê°œ ìš©ì–´ ì¸ì‹\")\n",
    "    if entities:\n",
    "        for entity in entities:\n",
    "            print(f\"  - {entity['term']} ({entity['label']}, {entity['confidence']:.2%})\")\n",
    "    else:\n",
    "        print(\"  (ì¸ì‹ëœ ê²Œì„ ìš©ì–´ ì—†ìŒ)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44018356-8b10-4e12-8fd0-7a19e828eb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ê²Œì„ ìš©ì–´ ë¶„ì„ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...\n",
      "   ë””ë°”ì´ìŠ¤: cuda\n",
      "   NER ëª¨ë¸ ë¡œë”©: models\\final_model\n",
      "    NER ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ë ˆì´ë¸” ìˆ˜: 39)\n",
      "   ìš©ì–´ ì‚¬ì „ ë¡œë”©: ìš©ì–´.json\n",
      "   ìš©ì–´ ì‚¬ì „ ë¡œë“œ ì™„ë£Œ (ê³ ìœ  ìš©ì–´: 86,994ê°œ)\n",
      " ì´ˆê¸°í™” ì™„ë£Œ!\n",
      "\n",
      "\n",
      "ê²Œì„ ìš©ì–´ê°€ í¬í•¨ëœ ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”\n",
      "ì¢…ë£Œí•˜ë ¤ë©´ që¥¼ ì…ë ¥í•˜ì„¸ìš”\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " ë¬¸ì¥ ì…ë ¥:  ë¡œì•„ ì¹´ì‚¬ë”˜ì€ ë‹¨ë‹¨í•˜ë‹¤\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ“ ì…ë ¥: ë¡œì•„ ì¹´ì‚¬ë”˜ì€ ë‹¨ë‹¨í•˜ë‹¤\n",
      "======================================================================\n",
      " ì¸ì‹ëœ ìš©ì–´: 1ê°œ\n",
      "\n",
      "[1] ë¡œì•„\n",
      "    ìœ í˜•: ëª¬ìŠ¤í„°\n",
      "    ì‹ ë¢°ë„: 37.1%\n",
      "    ì •ì˜: ë¡œìŠ¤íŠ¸ì•„í¬ ê²Œì„ì—ì„œ, ê²Œì„ì˜ ëª…ì¹­ì¸ 'ë¡œìŠ¤íŠ¸ì•„í¬'ì˜ ì¤€ë§.\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      " ë¬¸ì¥ ì…ë ¥:  ëƒ¥ê¾¼ì€ í•˜ìŠ¤ì—ì„œë„ ì‚¬ê¸°ì•¼?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ“ ì…ë ¥: ëƒ¥ê¾¼ì€ í•˜ìŠ¤ì—ì„œë„ ì‚¬ê¸°ì•¼?\n",
      "======================================================================\n",
      " ì¸ì‹ëœ ìš©ì–´: 1ê°œ\n",
      "\n",
      "[1] ëƒ¥ê¾¼\n",
      "    ìœ í˜•: ì§ì—…\n",
      "    ì‹ ë¢°ë„: 87.4%\n",
      "    ì •ì˜: ì›”ë“œì˜¤ë¸Œì›Œí¬ë˜í”„íŠ¸ ê²Œì„ì— ë“±ì¥í•˜ëŠ”, ìºë¦­í„°ì˜ ì§ì—…ì¸ 'ì‚¬ëƒ¥ê¾¼'ì˜ ì¤€ë§.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "\n",
    "class GameTermAnalyzer:\n",
    "    \n",
    "    def __init__(self, model_path: str, term_dict_path: str):\n",
    "        self.model_path = Path(model_path)\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        \n",
    "        print(f\" ê²Œì„ ìš©ì–´ ë¶„ì„ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...\")\n",
    "        print(f\"   ë””ë°”ì´ìŠ¤: {self.device}\")\n",
    "        \n",
    "        self._load_ner_model()     \n",
    "        self._load_term_dictionary(term_dict_path)\n",
    "        \n",
    "        print(f\" ì´ˆê¸°í™” ì™„ë£Œ!\\n\")\n",
    "    \n",
    "    def _load_ner_model(self):\n",
    "        print(f\"   NER ëª¨ë¸ ë¡œë”©: {self.model_path}\")\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)\n",
    "        self.model = AutoModelForTokenClassification.from_pretrained(self.model_path)\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # ë ˆì´ë¸” ë§µ ë¡œë“œ\n",
    "        label_map_path = self.model_path / \"label_map.json\"\n",
    "        if label_map_path.exists():\n",
    "            with open(label_map_path, 'r', encoding='utf-8') as f:\n",
    "                label_map = json.load(f)\n",
    "                self.id2label = {int(k): v for k, v in label_map['id2label'].items()}\n",
    "        else:\n",
    "            self.id2label = self.model.config.id2label\n",
    "        \n",
    "        print(f\"    NER ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ë ˆì´ë¸” ìˆ˜: {len(self.id2label)})\")\n",
    "    \n",
    "    def _load_term_dictionary(self, term_dict_path: str):\n",
    "        term_dict_path = Path(term_dict_path)\n",
    "        print(f\"   ìš©ì–´ ì‚¬ì „ ë¡œë”©: {term_dict_path.name}\")\n",
    "        \n",
    "        if not term_dict_path.exists():\n",
    "            raise FileNotFoundError(f\"ìš©ì–´ ì‚¬ì „ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {term_dict_path}\")\n",
    "        \n",
    "        with open(term_dict_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        self.terms = {}\n",
    "        for item in data:\n",
    "            term = item['term']\n",
    "            if term not in self.terms:\n",
    "                self.terms[term] = []\n",
    "            \n",
    "            self.terms[term].append({\n",
    "                'definition': item['definition'],\n",
    "                'facet': item.get('facet', ''),\n",
    "                'game': item.get('level3', ''),\n",
    "            })\n",
    "        \n",
    "        print(f\"   ìš©ì–´ ì‚¬ì „ ë¡œë“œ ì™„ë£Œ (ê³ ìœ  ìš©ì–´: {len(self.terms):,}ê°œ)\")\n",
    "    \n",
    "    def extract_entities(self, sentence: str, confidence_threshold: float = 0.0):\n",
    "        inputs = self.tokenizer(\n",
    "            sentence, \n",
    "            return_tensors=\"pt\", \n",
    "            truncation=True, \n",
    "            max_length=512\n",
    "        )\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            predictions = torch.argmax(outputs.logits, dim=2)\n",
    "            probabilities = torch.softmax(outputs.logits, dim=2)\n",
    "        \n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "        predictions = predictions[0].cpu().numpy()\n",
    "        probabilities = probabilities[0].cpu().numpy()\n",
    "        \n",
    "        entities = []\n",
    "        current_entity = None\n",
    "        \n",
    "        for i, (token, pred_id) in enumerate(zip(tokens, predictions)):\n",
    "            if token in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "                continue\n",
    "            \n",
    "            label = self.id2label[pred_id]\n",
    "            confidence = probabilities[i][pred_id]\n",
    "            \n",
    "            if label.startswith('B-'):\n",
    "                if current_entity and current_entity['confidence'] >= confidence_threshold:\n",
    "                    entities.append(current_entity)\n",
    "                \n",
    "                entity_type = label[2:]\n",
    "                current_entity = {\n",
    "                    'term': token.replace('##', ''),\n",
    "                    'facet': entity_type,\n",
    "                    'confidence': confidence\n",
    "                }\n",
    "            \n",
    "            elif label.startswith('I-') and current_entity:\n",
    "                entity_type = label[2:]\n",
    "                if entity_type == current_entity['facet']:\n",
    "                    current_entity['term'] += token.replace('##', '')\n",
    "                    current_entity['confidence'] = (current_entity['confidence'] + confidence) / 2\n",
    "            \n",
    "            else:\n",
    "                if current_entity and current_entity['confidence'] >= confidence_threshold:\n",
    "                    entities.append(current_entity)\n",
    "                    current_entity = None\n",
    "        \n",
    "        if current_entity and current_entity['confidence'] >= confidence_threshold:\n",
    "            entities.append(current_entity)\n",
    "        \n",
    "        return entities\n",
    "    \n",
    "    def get_definition(self, term: str, facet: Optional[str] = None) -> Optional[str]:\n",
    "        interpretations = self.terms.get(term)\n",
    "        \n",
    "        if not interpretations:\n",
    "            return None\n",
    "        \n",
    "        if facet and len(interpretations) > 1:\n",
    "            filtered = [i for i in interpretations if i['facet'] == facet]\n",
    "            if filtered:\n",
    "                return filtered[0]['definition']\n",
    "        \n",
    "        return interpretations[0]['definition']\n",
    "    \n",
    "    def analyze(self, sentence: str, confidence_threshold: float = 0.3):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"ğŸ“ ì…ë ¥: {sentence}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        entities = self.extract_entities(sentence, confidence_threshold)\n",
    "        \n",
    "        if not entities:\n",
    "            print(\" ì¸ì‹ëœ ê²Œì„ ìš©ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "            return\n",
    "        \n",
    "        print(f\" ì¸ì‹ëœ ìš©ì–´: {len(entities)}ê°œ\\n\")\n",
    "        \n",
    "        for idx, entity in enumerate(entities, 1):\n",
    "            term = entity['term']\n",
    "            facet = entity['facet']\n",
    "            confidence = entity['confidence']\n",
    "            \n",
    "            print(f\"[{idx}] {term}\")\n",
    "            print(f\"    ìœ í˜•: {facet}\")\n",
    "            print(f\"    ì‹ ë¢°ë„: {confidence:.1%}\")\n",
    "            \n",
    "            definition = self.get_definition(term, facet)\n",
    "            if definition:\n",
    "                print(f\"    ì •ì˜: {definition}\")\n",
    "            else:\n",
    "                print(f\"    ì •ì˜: (ì‚¬ì „ì— ë¯¸ë“±ë¡ëœ ìš©ì–´)\")\n",
    "            print()\n",
    "    \n",
    "    def batch_analyze(self, sentences: List[str], confidence_threshold: float = 0.3):\n",
    "        for i, sentence in enumerate(sentences, 1):\n",
    "            print(f\"\\n\\n{'#'*70}\")\n",
    "            print(f\"# ë¬¸ì¥ {i}/{len(sentences)}\")\n",
    "            print(f\"{'#'*70}\")\n",
    "            self.analyze(sentence, confidence_threshold)\n",
    "\n",
    "\n",
    "def main():\n",
    "    MODEL_PATH = Path(\"models/final_model\")\n",
    "    TERM_DICT_PATH = Path(\"dataset/ìš©ì–´.json\")\n",
    "    analyzer = GameTermAnalyzer(MODEL_PATH, TERM_DICT_PATH)\n",
    "    \n",
    "    print(\"\\nê²Œì„ ìš©ì–´ê°€ í¬í•¨ëœ ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”\")\n",
    "    print(\"ì¢…ë£Œí•˜ë ¤ë©´ që¥¼ ì…ë ¥í•˜ì„¸ìš”\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"\\n ë¬¸ì¥ ì…ë ¥: \").strip()\n",
    "            \n",
    "            if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'q']:\n",
    "                print(\"\\n í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "                break\n",
    "            \n",
    "            if not user_input:\n",
    "                print(\"  ë¬¸ì¥ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.\")\n",
    "                continue\n",
    "            \n",
    "            analyzer.analyze(user_input)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\n í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\n ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fabe123-11d3-4f53-b9ce-1738424529fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (NLP)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
