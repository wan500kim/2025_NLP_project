"""
ê²Œì„ ìš©ì–´ ë¶„ì„ ì‹œìŠ¤í…œ (í†µí•© ë²„ì „)
ë¬¸ì¥ì—ì„œ ê²Œì„ ìš©ì–´ë¥¼ ì¸ì‹í•˜ê³  ê°„ê²°í•˜ê²Œ í•´ì„ ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.
"""

import json
import torch
from pathlib import Path
from typing import Dict, List, Optional
from transformers import AutoTokenizer, AutoModelForTokenClassification


class GameTermAnalyzer:
    """ê²Œì„ ìš©ì–´ ë¶„ì„ê¸° (NER + í•´ì„ í†µí•©)"""
    
    def __init__(self, model_path: str, term_dict_path: str):
        """
        Args:
            model_path: í•™ìŠµëœ NER ëª¨ë¸ ê²½ë¡œ
            term_dict_path: ìš©ì–´.json íŒŒì¼ ê²½ë¡œ
        """
        self.model_path = Path(model_path)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        print(f"ğŸš€ ê²Œì„ ìš©ì–´ ë¶„ì„ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        print(f"   ë””ë°”ì´ìŠ¤: {self.device}")
        
        # NER ëª¨ë¸ ë¡œë“œ
        self._load_ner_model()
        
        # ìš©ì–´ ì‚¬ì „ ë¡œë“œ
        self._load_term_dictionary(term_dict_path)
        
        print(f"âœ… ì´ˆê¸°í™” ì™„ë£Œ!\n")
    
    def _load_ner_model(self):
        """NER ëª¨ë¸ ë¡œë“œ"""
        print(f"   NER ëª¨ë¸ ë¡œë”©: {self.model_path}")
        
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
        self.model = AutoModelForTokenClassification.from_pretrained(self.model_path)
        self.model.to(self.device)
        self.model.eval()
        
        # ë ˆì´ë¸” ë§µ ë¡œë“œ
        label_map_path = self.model_path / "label_map.json"
        if label_map_path.exists():
            with open(label_map_path, 'r', encoding='utf-8') as f:
                label_map = json.load(f)
                self.id2label = {int(k): v for k, v in label_map['id2label'].items()}
        else:
            self.id2label = self.model.config.id2label
        
        print(f"   âœ“ NER ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ë ˆì´ë¸” ìˆ˜: {len(self.id2label)})")
    
    def _load_term_dictionary(self, term_dict_path: str):
        """ìš©ì–´ ì‚¬ì „ ë¡œë“œ"""
        term_dict_path = Path(term_dict_path)
        print(f"   ìš©ì–´ ì‚¬ì „ ë¡œë”©: {term_dict_path.name}")
        
        if not term_dict_path.exists():
            raise FileNotFoundError(f"ìš©ì–´ ì‚¬ì „ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {term_dict_path}")
        
        with open(term_dict_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # ìš©ì–´ë¥¼ í‚¤ë¡œ í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        self.terms = {}
        for item in data:
            term = item['term']
            if term not in self.terms:
                self.terms[term] = []
            
            self.terms[term].append({
                'definition': item['definition'],
                'facet': item.get('facet', ''),
                'game': item.get('level3', ''),
            })
        
        print(f"   âœ“ ìš©ì–´ ì‚¬ì „ ë¡œë“œ ì™„ë£Œ (ê³ ìœ  ìš©ì–´: {len(self.terms):,}ê°œ)")
    
    def extract_entities(self, sentence: str, confidence_threshold: float = 0.0):
        """
        ë¬¸ì¥ì—ì„œ ê²Œì„ ìš©ì–´ ì¶”ì¶œ
        
        Args:
            sentence: ì…ë ¥ ë¬¸ì¥
            confidence_threshold: ìµœì†Œ ì‹ ë¢°ë„ (0.0 ~ 1.0)
            
        Returns:
            ì¶”ì¶œëœ ì—”í‹°í‹° ë¦¬ìŠ¤íŠ¸
        """
        # í† í¬ë‚˜ì´ì§•
        inputs = self.tokenizer(
            sentence, 
            return_tensors="pt", 
            truncation=True, 
            max_length=512
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        # ì˜ˆì¸¡
        with torch.no_grad():
            outputs = self.model(**inputs)
            predictions = torch.argmax(outputs.logits, dim=2)
            probabilities = torch.softmax(outputs.logits, dim=2)
        
        # í† í°ê³¼ ì˜ˆì¸¡ ê²°ê³¼
        tokens = self.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
        predictions = predictions[0].cpu().numpy()
        probabilities = probabilities[0].cpu().numpy()
        
        # ì—”í‹°í‹° ì¶”ì¶œ
        entities = []
        current_entity = None
        
        for i, (token, pred_id) in enumerate(zip(tokens, predictions)):
            if token in ['[CLS]', '[SEP]', '[PAD]']:
                continue
            
            label = self.id2label[pred_id]
            confidence = probabilities[i][pred_id]
            
            if label.startswith('B-'):
                if current_entity and current_entity['confidence'] >= confidence_threshold:
                    entities.append(current_entity)
                
                entity_type = label[2:]
                current_entity = {
                    'term': token.replace('##', ''),
                    'facet': entity_type,
                    'confidence': confidence
                }
            
            elif label.startswith('I-') and current_entity:
                entity_type = label[2:]
                if entity_type == current_entity['facet']:
                    current_entity['term'] += token.replace('##', '')
                    current_entity['confidence'] = (current_entity['confidence'] + confidence) / 2
            
            else:  # 'O' íƒœê·¸
                if current_entity and current_entity['confidence'] >= confidence_threshold:
                    entities.append(current_entity)
                    current_entity = None
        
        if current_entity and current_entity['confidence'] >= confidence_threshold:
            entities.append(current_entity)
        
        return entities
    
    def get_definition(self, term: str, facet: Optional[str] = None) -> Optional[str]:
        """
        ìš©ì–´ ì •ì˜ ê°€ì ¸ì˜¤ê¸°
        
        Args:
            term: ìš©ì–´
            facet: ìš©ì–´ ìœ í˜• (í•„í„°ë§ìš©)
            
        Returns:
            ìš©ì–´ ì •ì˜ ë˜ëŠ” None
        """
        interpretations = self.terms.get(term)
        
        if not interpretations:
            return None
        
        # facetìœ¼ë¡œ í•„í„°ë§
        if facet and len(interpretations) > 1:
            filtered = [i for i in interpretations if i['facet'] == facet]
            if filtered:
                return filtered[0]['definition']
        
        return interpretations[0]['definition']
    
    def analyze(self, sentence: str, confidence_threshold: float = 0.3):
        """
        ë¬¸ì¥ ë¶„ì„ ë° ê²°ê³¼ ì¶œë ¥
        
        Args:
            sentence: ì…ë ¥ ë¬¸ì¥
            confidence_threshold: ìµœì†Œ ì‹ ë¢°ë„
        """
        print(f"\n{'='*70}")
        print(f"ğŸ“ ì…ë ¥: {sentence}")
        print(f"{'='*70}")
        
        # ìš©ì–´ ì¶”ì¶œ
        entities = self.extract_entities(sentence, confidence_threshold)
        
        if not entities:
            print("âŒ ì¸ì‹ëœ ê²Œì„ ìš©ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"âœ… ì¸ì‹ëœ ìš©ì–´: {len(entities)}ê°œ\n")
        
        for idx, entity in enumerate(entities, 1):
            term = entity['term']
            facet = entity['facet']
            confidence = entity['confidence']
            
            print(f"[{idx}] {term}")
            print(f"    ìœ í˜•: {facet}")
            print(f"    ì‹ ë¢°ë„: {confidence:.1%}")
            
            # ì •ì˜ ê°€ì ¸ì˜¤ê¸°
            definition = self.get_definition(term, facet)
            if definition:
                print(f"    ì •ì˜: {definition}")
            else:
                print(f"    ì •ì˜: (ì‚¬ì „ì— ë¯¸ë“±ë¡ëœ ìš©ì–´)")
            print()
    
    def batch_analyze(self, sentences: List[str], confidence_threshold: float = 0.3):
        """
        ì—¬ëŸ¬ ë¬¸ì¥ ì¼ê´„ ë¶„ì„
        
        Args:
            sentences: ë¬¸ì¥ ë¦¬ìŠ¤íŠ¸
            confidence_threshold: ìµœì†Œ ì‹ ë¢°ë„
        """
        for i, sentence in enumerate(sentences, 1):
            print(f"\n\n{'#'*70}")
            print(f"# ë¬¸ì¥ {i}/{len(sentences)}")
            print(f"{'#'*70}")
            self.analyze(sentence, confidence_threshold)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # ========== ê²½ë¡œ ì„¤ì • ==========
    MODEL_PATH = r"C:\Users\dhkst\OneDrive\ë¬¸ì„œ\GitHub\2025_NLP_project\models\final_model"
    TERM_DICT_PATH = r"C:\Users\dhkst\OneDrive\ë°”íƒ• í™”ë©´\ë‚´êº¼\ëŒ€\4-2\ìì—°ì–¸ì–´ì²˜ë¦¬\160.ë¬¸í™”, ê²Œì„ ì½˜í…ì¸  ë¶„ì•¼ ìš©ì–´ ë§ë­‰ì¹˜\01-1.ì •ì‹ê°œë°©ë°ì´í„°\Training\02.ë¼ë²¨ë§ë°ì´í„°\TL\ìš©ì–´.json"
    
    # ========== ì‹œìŠ¤í…œ ì´ˆê¸°í™” ==========
    analyzer = GameTermAnalyzer(MODEL_PATH, TERM_DICT_PATH)
    
    # ========== ì˜ˆì‹œ 1: ë‹¨ì¼ ë¬¸ì¥ ë¶„ì„ ==========
    print("\n" + "="*70)
    print("ì˜ˆì‹œ 1: ë‹¨ì¼ ë¬¸ì¥ ë¶„ì„")
    print("="*70)
    
    analyzer.analyze("ë¼ì´ì•„ê°€ ë„ˆë¬´ ê°•í•´ì„œ ê³µëµì´ ì–´ë ¤ì›Œìš”")
    
    # ========== ì˜ˆì‹œ 2: ì—¬ëŸ¬ ë¬¸ì¥ ë¶„ì„ ==========
    print("\n\n" + "="*70)
    print("ì˜ˆì‹œ 2: ì—¬ëŸ¬ ë¬¸ì¥ ì¼ê´„ ë¶„ì„")
    print("="*70)
    
    test_sentences = [
        "ë°©ë°˜ ê°€ê²©ì´ ë„ˆë¬´ ë¹„ì‹¸ìš”",
        "ìŠ¤ëª¨ì»¤ì˜ ì„±ì¥ ë¬¼ì•½ì„ ì‚¬ìš©í•˜ë©´ ê²½í—˜ì¹˜ê°€ 7ë°° ì˜¬ë¼ìš”",
        "ê°€ë””ì–¸ì„ ì¡ìœ¼ë ¤ë©´ ë¶ˆ ì†ì„± ê³µê²©ì´ í•„ìš”í•´ìš”"
    ]
    
    analyzer.batch_analyze(test_sentences)
    
    # ========== ì˜ˆì‹œ 3: ëŒ€í™”í˜• ëª¨ë“œ ==========
    print("\n\n" + "="*70)
    print("ì˜ˆì‹œ 3: ëŒ€í™”í˜• ëª¨ë“œ")
    print("="*70)
    print("\nê²Œì„ ìš©ì–´ê°€ í¬í•¨ëœ ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”")
    print("ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ë˜ëŠ” 'exit'ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
    print("="*70)
    
    while True:
        try:
            user_input = input("\nğŸ“ ë¬¸ì¥ ì…ë ¥: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'q']:
                print("\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            if not user_input:
                print("âš ï¸  ë¬¸ì¥ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue
            
            analyzer.analyze(user_input)
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == "__main__":
    main()
