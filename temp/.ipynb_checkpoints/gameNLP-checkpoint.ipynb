{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa01f5a3-8349-45f5-b864-d25a217f5b68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA 사용 가능: True\n",
      "GPU: NVIDIA GeForce RTX 2060\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import random\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "MODEL_NAME = \"monologg/koelectra-base-v3-discriminator\"\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 5e-5\n",
    "\n",
    "print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
    "print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "111e14d2-e78b-46de-b699-c381c8d529a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_file = Path(\"dataset/TL/용례_게임tl.json\")\n",
    "val_input_file = Path(\"dataset/VL/용례_게임vl.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5894c96e-1129-4a42-8844-7ad13f9c8736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON 파일 로딩 중...\n",
      "총 199,504개 샘플 발견\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training examples: 100%|█████████████████████████████████████████| 199504/199504 [00:02<00:00, 73609.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터: 199,504개 예제 처리 완료\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_processed_data = []\n",
    "labels = set(['O'])\n",
    "\n",
    "print(\"JSON 파일 로딩 중...\")\n",
    "with open(train_input_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "total_samples = len(data)\n",
    "print(f\"총 {total_samples:,}개 샘플 발견\")\n",
    "\n",
    "for example in tqdm(data, desc=\"Processing training examples\"):\n",
    "    sentence = example.get('sentence', '')\n",
    "    tokens = example.get('tokens', [])\n",
    "    \n",
    "    if sentence and tokens:\n",
    "        char_tags = ['O'] * len(sentence)\n",
    "        \n",
    "        for token in tokens:\n",
    "            start = token['start']\n",
    "            length = token['length']\n",
    "            facet = token.get('facet', 'TERM')\n",
    "            \n",
    "            if start < len(sentence):\n",
    "                char_tags[start] = f'B-{facet}'\n",
    "            \n",
    "            for i in range(start + 1, start + length):\n",
    "                if i < len(sentence):\n",
    "                    char_tags[i] = f'I-{facet}'\n",
    "        \n",
    "        chars = list(sentence)\n",
    "        tags = char_tags\n",
    "        labels.update(tags)\n",
    "        \n",
    "        train_processed_data.append({\n",
    "            'id': example.get('id'),\n",
    "            'sentence': sentence,\n",
    "            'chars': chars,\n",
    "            'tags': tags,\n",
    "            'tokens': tokens\n",
    "        })\n",
    "\n",
    "# 메모리 정리\n",
    "del data\n",
    "\n",
    "print(f\"학습 데이터: {len(train_processed_data):,}개 예제 처리 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c7e5120-f274-4886-9beb-1e0f6fecc34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "검증 데이터 처리 중: dataset\\VL\\용례_게임vl.json\n",
      "JSON 파일 로딩 중...\n",
      "총 24,938개 샘플 발견\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing validation examples: 100%|████████████████████████████████████████| 24938/24938 [00:00<00:00, 110602.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터: 24,938개 예제 처리 완료\n",
      "\n",
      "데이터 분할 완료:\n",
      "  - Train: 179,553 samples\n",
      "  - Val: 24,938 samples\n",
      "  - Test: 19,951 samples\n"
     ]
    }
   ],
   "source": [
    "if val_input_file and val_input_file.exists():\n",
    "    print(f\"\\n검증 데이터 처리 중: {val_input_file}\")\n",
    "    val_processed_data = []\n",
    "    \n",
    "    print(\"JSON 파일 로딩 중...\")\n",
    "    with open(val_input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    total_samples = len(data)\n",
    "    print(f\"총 {total_samples:,}개 샘플 발견\")\n",
    "    \n",
    "    for example in tqdm(data, desc=\"Processing validation examples\"):\n",
    "        sentence = example.get('sentence', '')\n",
    "        tokens = example.get('tokens', [])\n",
    "        \n",
    "        if sentence and tokens:\n",
    "            char_tags = ['O'] * len(sentence)\n",
    "            \n",
    "            for token in tokens:\n",
    "                start = token['start']\n",
    "                length = token['length']\n",
    "                facet = token.get('facet', 'TERM')\n",
    "                \n",
    "                if start < len(sentence):\n",
    "                    char_tags[start] = f'B-{facet}'\n",
    "                \n",
    "                for i in range(start + 1, start + length):\n",
    "                    if i < len(sentence):\n",
    "                        char_tags[i] = f'I-{facet}'\n",
    "            \n",
    "            chars = list(sentence)\n",
    "            tags = char_tags\n",
    "            labels.update(tags)\n",
    "            \n",
    "            val_processed_data.append({\n",
    "                'id': example.get('id'),\n",
    "                'sentence': sentence,\n",
    "                'chars': chars,\n",
    "                'tags': tags,\n",
    "                'tokens': tokens\n",
    "            })\n",
    "    \n",
    "    del data\n",
    "    \n",
    "    print(f\"검증 데이터: {len(val_processed_data):,}개 예제 처리 완료\")\n",
    "    \n",
    "    random.shuffle(train_processed_data)\n",
    "    train_size = int(len(train_processed_data) * 0.9)\n",
    "    \n",
    "    train_data = train_processed_data[:train_size]\n",
    "    test_data = train_processed_data[train_size:]\n",
    "    val_data = val_processed_data\n",
    "    \n",
    "else:\n",
    "    print(\"\\n검증 파일이 없어 학습 데이터를 분할합니다.\")\n",
    "    random.shuffle(train_processed_data)\n",
    "    total = len(train_processed_data)\n",
    "    train_size = int(total * 0.8)\n",
    "    val_size = int(total * 0.1)\n",
    "    \n",
    "    train_data = train_processed_data[:train_size]\n",
    "    val_data = train_processed_data[train_size:train_size + val_size]\n",
    "    test_data = train_processed_data[train_size + val_size:]\n",
    "\n",
    "print(f\"\\n데이터 분할 완료:\")\n",
    "print(f\"  - Train: {len(train_data):,} samples\")\n",
    "print(f\"  - Val: {len(val_data):,} samples\")\n",
    "print(f\"  - Test: {len(test_data):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cae24ce1-36c6-4a25-944f-1be75fa70bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 완료! 데이터 저장됨\n"
     ]
    }
   ],
   "source": [
    "with open(\"processed/train.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "with open(\"processed/val.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(val_data, f, ensure_ascii=False, indent=2)\n",
    "with open(\"processed/test.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(test_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "label2id = {label: idx for idx, label in enumerate(sorted(labels))}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "label_map = {\n",
    "    'label2id': label2id,\n",
    "    'id2label': id2label,\n",
    "    'num_labels': len(labels)\n",
    "}\n",
    "\n",
    "with open(\"processed/label_map.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(label_map, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"전처리 완료! 데이터 저장됨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4cdbf4a7-922a-4b5e-8ea4-2baa5360e164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "레이블 수: 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 초기화 완료!\n"
     ]
    }
   ],
   "source": [
    "with open(\"processed/label_map.json\", 'r', encoding='utf-8') as f:\n",
    "    label_map = json.load(f)\n",
    "\n",
    "label2id = label_map['label2id']\n",
    "id2label = {int(k): v for k, v in label_map['id2label'].items()}\n",
    "num_labels = label_map['num_labels']\n",
    "\n",
    "print(f\"레이블 수: {num_labels}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    use_safetensors=True\n",
    ")\n",
    "\n",
    "print(\"모델 초기화 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "853c5c39-5ca0-4f73-b0ee-fbbf0d52338f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 로딩 중...\n",
      "  - Train: 179553 samples\n",
      "  - Val: 24938 samples\n",
      "  - Test: 19951 samples\n",
      "\n",
      "토큰화 중...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e03b92eec94f2f9924d791151b9960",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/179553 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85f918d812e842749b21fd776cd3c0f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24938 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화 완료!\n"
     ]
    }
   ],
   "source": [
    "print(\"데이터 로딩 중...\")\n",
    "with open(\"processed/train.json\", 'r', encoding='utf-8') as f:\n",
    "    train_data = json.load(f)\n",
    "with open(\"processed/val.json\", 'r', encoding='utf-8') as f:\n",
    "    val_data = json.load(f)\n",
    "with open(\"processed/test.json\", 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"  - Train: {len(train_data)} samples\")\n",
    "print(f\"  - Val: {len(val_data)} samples\")\n",
    "print(f\"  - Test: {len(test_data)} samples\")\n",
    "\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "print(\"\\n토큰화 중...\")\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['chars'],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label2id[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_train = train_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "tokenized_val = val_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names\n",
    ")\n",
    "\n",
    "print(\"토큰화 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ec6ebd0-7f84-4734-ba0a-6234f3bded0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    true_predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        for pred_id, label_id in zip(prediction, label):\n",
    "            if label_id != -100:\n",
    "                true_predictions.append(pred_id)\n",
    "                true_labels.append(label_id)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, \n",
    "        true_predictions, \n",
    "        average='weighted',\n",
    "        zero_division=0\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "18742cda-2ce7-4e6d-8ca6-d0ce0774cd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "모델 학습 시작...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dhkst\\AppData\\Local\\Temp\\ipykernel_18452\\3999087097.py:17: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33669' max='33669' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33669/33669 2:34:41, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.330000</td>\n",
       "      <td>0.324001</td>\n",
       "      <td>0.886310</td>\n",
       "      <td>0.896017</td>\n",
       "      <td>0.888616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.265000</td>\n",
       "      <td>0.273065</td>\n",
       "      <td>0.902674</td>\n",
       "      <td>0.907787</td>\n",
       "      <td>0.903903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.204600</td>\n",
       "      <td>0.260626</td>\n",
       "      <td>0.908595</td>\n",
       "      <td>0.912654</td>\n",
       "      <td>0.910044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "학습 완료! 모델 저장: models/final_model\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n모델 학습 시작...\")\n",
    "\n",
    "Path(\"models\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"models/final_model\")\n",
    "tokenizer.save_pretrained(\"models/final_model\")\n",
    "\n",
    "print(f\"\\n학습 완료! 모델 저장: models/final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba02620-f870-4762-b16b-8d0cdef627bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8762b63-259f-482b-b1b8-ca5b0411b536",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11 (NLP)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
