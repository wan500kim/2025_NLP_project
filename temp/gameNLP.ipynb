{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa01f5a3-8349-45f5-b864-d25a217f5b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import random\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "MODEL_NAME = \"monologg/koelectra-base-v3-discriminator\"\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 16\n",
    "LEARNING_RATE = 5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "111e14d2-e78b-46de-b699-c381c8d529a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input_file = Path(\"dataset/TL/용례_게임tl.json\")\n",
    "val_input_file = Path(\"dataset/VL/용례_게임vl.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5894c96e-1129-4a42-8844-7ad13f9c8736",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON 파일 로딩 중...\n",
      "총 199,504개 샘플 발견\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing training examples: 100%|█████████████████████████████████████████| 199504/199504 [00:02<00:00, 66696.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 데이터: 199,504개 예제 처리 완료\n"
     ]
    }
   ],
   "source": [
    "train_processed_data = []\n",
    "labels = set(['O'])\n",
    "\n",
    "print(\"JSON 파일 로딩 중...\")\n",
    "with open(train_input_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "total_samples = len(data)\n",
    "print(f\"총 {total_samples:,}개 샘플 발견\")\n",
    "\n",
    "for example in tqdm(data, desc=\"Processing training examples\"):\n",
    "    sentence = example.get('sentence', '')\n",
    "    tokens = example.get('tokens', [])\n",
    "    \n",
    "    if sentence and tokens:\n",
    "        char_tags = ['O'] * len(sentence)\n",
    "        \n",
    "        for token in tokens:\n",
    "            start = token['start']\n",
    "            length = token['length']\n",
    "            facet = token.get('facet', 'TERM')\n",
    "            \n",
    "            if start < len(sentence):\n",
    "                char_tags[start] = f'B-{facet}'\n",
    "            \n",
    "            for i in range(start + 1, start + length):\n",
    "                if i < len(sentence):\n",
    "                    char_tags[i] = f'I-{facet}'\n",
    "        \n",
    "        chars = list(sentence)\n",
    "        tags = char_tags\n",
    "        labels.update(tags)\n",
    "        \n",
    "        train_processed_data.append({\n",
    "            'id': example.get('id'),\n",
    "            'sentence': sentence,\n",
    "            'chars': chars,\n",
    "            'tags': tags,\n",
    "            'tokens': tokens\n",
    "        })\n",
    "\n",
    "# 메모리 정리\n",
    "del data\n",
    "\n",
    "print(f\"학습 데이터: {len(train_processed_data):,}개 예제 처리 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c7e5120-f274-4886-9beb-1e0f6fecc34e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "검증 데이터 처리 중: dataset\\VL\\용례_게임vl.json\n",
      "JSON 파일 로딩 중...\n",
      "총 24,938개 샘플 발견\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing validation examples: 100%|█████████████████████████████████████████| 24938/24938 [00:00<00:00, 83839.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "검증 데이터: 24,938개 예제 처리 완료\n",
      "\n",
      "데이터 분할 완료:\n",
      "  - Train: 179,553 samples\n",
      "  - Val: 24,938 samples\n",
      "  - Test: 19,951 samples\n"
     ]
    }
   ],
   "source": [
    "if val_input_file and val_input_file.exists():\n",
    "    print(f\"\\n검증 데이터 처리 중: {val_input_file}\")\n",
    "    val_processed_data = []\n",
    "    \n",
    "    print(\"JSON 파일 로딩 중...\")\n",
    "    with open(val_input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    total_samples = len(data)\n",
    "    print(f\"총 {total_samples:,}개 샘플 발견\")\n",
    "    \n",
    "    for example in tqdm(data, desc=\"Processing validation examples\"):\n",
    "        sentence = example.get('sentence', '')\n",
    "        tokens = example.get('tokens', [])\n",
    "        \n",
    "        if sentence and tokens:\n",
    "            char_tags = ['O'] * len(sentence)\n",
    "            \n",
    "            for token in tokens:\n",
    "                start = token['start']\n",
    "                length = token['length']\n",
    "                facet = token.get('facet', 'TERM')\n",
    "                \n",
    "                if start < len(sentence):\n",
    "                    char_tags[start] = f'B-{facet}'\n",
    "                \n",
    "                for i in range(start + 1, start + length):\n",
    "                    if i < len(sentence):\n",
    "                        char_tags[i] = f'I-{facet}'\n",
    "            \n",
    "            chars = list(sentence)\n",
    "            tags = char_tags\n",
    "            labels.update(tags)\n",
    "            \n",
    "            val_processed_data.append({\n",
    "                'id': example.get('id'),\n",
    "                'sentence': sentence,\n",
    "                'chars': chars,\n",
    "                'tags': tags,\n",
    "                'tokens': tokens\n",
    "            })\n",
    "    \n",
    "    del data\n",
    "    \n",
    "    print(f\"검증 데이터: {len(val_processed_data):,}개 예제 처리 완료\")\n",
    "    \n",
    "    random.shuffle(train_processed_data)\n",
    "    train_size = int(len(train_processed_data) * 0.9)\n",
    "    \n",
    "    train_data = train_processed_data[:train_size]\n",
    "    test_data = train_processed_data[train_size:]\n",
    "    val_data = val_processed_data\n",
    "    \n",
    "else:\n",
    "    print(\"\\n검증 파일이 없어 학습 데이터를 분할합니다.\")\n",
    "    random.shuffle(train_processed_data)\n",
    "    total = len(train_processed_data)\n",
    "    train_size = int(total * 0.8)\n",
    "    val_size = int(total * 0.1)\n",
    "    \n",
    "    train_data = train_processed_data[:train_size]\n",
    "    val_data = train_processed_data[train_size:train_size + val_size]\n",
    "    test_data = train_processed_data[train_size + val_size:]\n",
    "\n",
    "print(f\"\\n데이터 분할 완료:\")\n",
    "print(f\"  - Train: {len(train_data):,} samples\")\n",
    "print(f\"  - Val: {len(val_data):,} samples\")\n",
    "print(f\"  - Test: {len(test_data):,} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cae24ce1-36c6-4a25-944f-1be75fa70bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 완료! 데이터 저장됨\n"
     ]
    }
   ],
   "source": [
    "with open(\"processed/train.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(train_data, f, ensure_ascii=False, indent=2)\n",
    "with open(\"processed/val.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(val_data, f, ensure_ascii=False, indent=2)\n",
    "with open(\"processed/test.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(test_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "label2id = {label: idx for idx, label in enumerate(sorted(labels))}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "label_map = {\n",
    "    'label2id': label2id,\n",
    "    'id2label': id2label,\n",
    "    'num_labels': len(labels)\n",
    "}\n",
    "\n",
    "with open(\"processed/label_map.json\", 'w', encoding='utf-8') as f:\n",
    "    json.dump(label_map, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"전처리 완료! 데이터 저장됨\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cdbf4a7-922a-4b5e-8ea4-2baa5360e164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "레이블 수: 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ElectraForTokenClassification were not initialized from the model checkpoint at monologg/koelectra-base-v3-discriminator and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 초기화 완료!\n"
     ]
    }
   ],
   "source": [
    "with open(\"processed/label_map.json\", 'r', encoding='utf-8') as f:\n",
    "    label_map = json.load(f)\n",
    "\n",
    "label2id = label_map['label2id']\n",
    "id2label = {int(k): v for k, v in label_map['id2label'].items()}\n",
    "num_labels = label_map['num_labels']\n",
    "\n",
    "print(f\"레이블 수: {num_labels}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "print(\"모델 초기화 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "853c5c39-5ca0-4f73-b0ee-fbbf0d52338f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 로딩 중...\n",
      "  - Train: 179553 samples\n",
      "  - Val: 24938 samples\n",
      "  - Test: 19951 samples\n",
      "\n",
      "토큰화 중...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "996ac8debafe418e83cf4ab782972a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/179553 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b1f47fce4c4a53ad72f33db6b7fd48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24938 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "토큰화 완료!\n"
     ]
    }
   ],
   "source": [
    "print(\"데이터 로딩 중...\")\n",
    "with open(\"processed/train.json\", 'r', encoding='utf-8') as f:\n",
    "    train_data = json.load(f)\n",
    "with open(\"processed/val.json\", 'r', encoding='utf-8') as f:\n",
    "    val_data = json.load(f)\n",
    "with open(\"processed/test.json\", 'r', encoding='utf-8') as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "print(f\"  - Train: {len(train_data)} samples\")\n",
    "print(f\"  - Val: {len(val_data)} samples\")\n",
    "print(f\"  - Test: {len(test_data)} samples\")\n",
    "\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "val_dataset = Dataset.from_list(val_data)\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "print(\"\\n토큰화 중...\")\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples['chars'],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        max_length=512\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(examples['tags']):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        \n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label2id[label[word_idx]])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_train = train_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names\n",
    ")\n",
    "tokenized_val = val_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names\n",
    ")\n",
    "\n",
    "print(\"토큰화 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bff435d7-f605-47b4-a895-f13a0f81cf81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu126\n",
      "Requirement already satisfied: torch in c:\\users\\dhkst\\anaconda3\\lib\\site-packages (2.9.1)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cu126/torchvision-0.24.1%2Bcu126-cp313-cp313-win_amd64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\dhkst\\anaconda3\\lib\\site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\dhkst\\anaconda3\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\dhkst\\anaconda3\\lib\\site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\dhkst\\anaconda3\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\dhkst\\anaconda3\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\dhkst\\anaconda3\\lib\\site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\dhkst\\anaconda3\\lib\\site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\dhkst\\anaconda3\\lib\\site-packages (from torchvision) (2.1.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\dhkst\\anaconda3\\lib\\site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\dhkst\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\dhkst\\anaconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading https://download.pytorch.org/whl/cu126/torchvision-0.24.1%2Bcu126-cp313-cp313-win_amd64.whl (8.8 MB)\n",
      "   ---------------------------------------- 0.0/8.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 8.8/8.8 MB 50.6 MB/s eta 0:00:00\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.24.1+cu126\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision --index-url https://download.pytorch.org/whl/cu126"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa176396-4d3c-494e-a47f-44ec311ce8fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA 사용 가능: True\n",
      "현재 디바이스: NVIDIA GeForce RTX 2060\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"CUDA 사용 가능: {torch.cuda.is_available()}\")\n",
    "print(f\"현재 디바이스: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ec6ebd0-7f84-4734-ba0a-6234f3bded0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    \n",
    "    true_predictions = []\n",
    "    true_labels = []\n",
    "    \n",
    "    for prediction, label in zip(predictions, labels):\n",
    "        for pred_id, label_id in zip(prediction, label):\n",
    "            if label_id != -100:\n",
    "                true_predictions.append(pred_id)\n",
    "                true_labels.append(label_id)\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        true_labels, \n",
    "        true_predictions, \n",
    "        average='weighted',\n",
    "        zero_division=0\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18742cda-2ce7-4e6d-8ca6-d0ce0774cd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n모델 학습 시작...\")\n",
    "\n",
    "Path(\"models\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    num_train_epochs=NUM_EPOCHS,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"models/final_model\")\n",
    "tokenizer.save_pretrained(\"models/final_model\")\n",
    "\n",
    "print(f\"\\n학습 완료! 모델 저장: models/final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318bfcfd-5d01-4513-a6e5-cfc24a26073c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (gpunlp)",
   "language": "python",
   "name": "gpunlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
