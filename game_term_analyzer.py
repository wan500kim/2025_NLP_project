import json
import torch
from pathlib import Path
from typing import Dict, List, Optional
from transformers import AutoTokenizer, AutoModelForTokenClassification


class GameTermAnalyzer:
    
    def __init__(self, model_path: str, term_dict_path: str):
        self.model_path = Path(model_path)
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        print(f" ê²Œì„ ìš©ì–´ ë¶„ì„ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì¤‘...")
        print(f"   ë””ë°”ì´ìŠ¤: {self.device}")
        
        self._load_ner_model()     
        self._load_term_dictionary(term_dict_path)
        
        print(f" ì´ˆê¸°í™” ì™„ë£Œ!\n")
    
    def _load_ner_model(self):
        print(f"   NER ëª¨ë¸ ë¡œë”©: {self.model_path}")
        
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
        self.model = AutoModelForTokenClassification.from_pretrained(self.model_path)
        self.model.to(self.device)
        self.model.eval()
        
        # ë ˆì´ë¸” ë§µ ë¡œë“œ
        label_map_path = self.model_path / "label_map.json"
        if label_map_path.exists():
            with open(label_map_path, 'r', encoding='utf-8') as f:
                label_map = json.load(f)
                self.id2label = {int(k): v for k, v in label_map['id2label'].items()}
        else:
            self.id2label = self.model.config.id2label
        
        print(f"    NER ëª¨ë¸ ë¡œë“œ ì™„ë£Œ (ë ˆì´ë¸” ìˆ˜: {len(self.id2label)})")
    
    def _load_term_dictionary(self, term_dict_path: str):
        term_dict_path = Path(term_dict_path)
        print(f"   ìš©ì–´ ì‚¬ì „ ë¡œë”©: {term_dict_path.name}")
        
        if not term_dict_path.exists():
            raise FileNotFoundError(f"ìš©ì–´ ì‚¬ì „ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {term_dict_path}")
        
        with open(term_dict_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.terms = {}
        for item in data:
            term = item['term']
            if term not in self.terms:
                self.terms[term] = []
            
            self.terms[term].append({
                'definition': item['definition'],
                'facet': item.get('facet', ''),
                'game': item.get('level3', ''),
            })
        
        print(f"   ìš©ì–´ ì‚¬ì „ ë¡œë“œ ì™„ë£Œ (ê³ ìœ  ìš©ì–´: {len(self.terms):,}ê°œ)")
    
    def extract_entities(self, sentence: str, confidence_threshold: float = 0.0):
        inputs = self.tokenizer(
            sentence, 
            return_tensors="pt", 
            truncation=True, 
            max_length=512
        )
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            predictions = torch.argmax(outputs.logits, dim=2)
            probabilities = torch.softmax(outputs.logits, dim=2)
        
        tokens = self.tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
        predictions = predictions[0].cpu().numpy()
        probabilities = probabilities[0].cpu().numpy()
        
        entities = []
        current_entity = None
        
        for i, (token, pred_id) in enumerate(zip(tokens, predictions)):
            if token in ['[CLS]', '[SEP]', '[PAD]']:
                continue
            
            label = self.id2label[pred_id]
            confidence = probabilities[i][pred_id]
            
            if label.startswith('B-'):
                if current_entity and current_entity['confidence'] >= confidence_threshold:
                    entities.append(current_entity)
                
                entity_type = label[2:]
                current_entity = {
                    'term': token.replace('##', ''),
                    'facet': entity_type,
                    'confidence': confidence
                }
            
            elif label.startswith('I-') and current_entity:
                entity_type = label[2:]
                if entity_type == current_entity['facet']:
                    current_entity['term'] += token.replace('##', '')
                    current_entity['confidence'] = (current_entity['confidence'] + confidence) / 2
            
            else:
                if current_entity and current_entity['confidence'] >= confidence_threshold:
                    entities.append(current_entity)
                    current_entity = None
        
        if current_entity and current_entity['confidence'] >= confidence_threshold:
            entities.append(current_entity)
        
        return entities
    
    def get_definition(self, term: str, facet: Optional[str] = None) -> Optional[str]:
        interpretations = self.terms.get(term)
        
        if not interpretations:
            return None
        
        if facet and len(interpretations) > 1:
            filtered = [i for i in interpretations if i['facet'] == facet]
            if filtered:
                return filtered[0]['definition']
        
        return interpretations[0]['definition']
    
    def analyze(self, sentence: str, confidence_threshold: float = 0.3):
        print(f"\n{'='*70}")
        print(f"ğŸ“ ì…ë ¥: {sentence}")
        print(f"{'='*70}")
        
        entities = self.extract_entities(sentence, confidence_threshold)
        
        if not entities:
            print(" ì¸ì‹ëœ ê²Œì„ ìš©ì–´ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return
        
        print(f" ì¸ì‹ëœ ìš©ì–´: {len(entities)}ê°œ\n")
        
        for idx, entity in enumerate(entities, 1):
            term = entity['term']
            facet = entity['facet']
            confidence = entity['confidence']
            
            print(f"[{idx}] {term}")
            print(f"    ìœ í˜•: {facet}")
            print(f"    ì‹ ë¢°ë„: {confidence:.1%}")
            
            definition = self.get_definition(term, facet)
            if definition:
                print(f"    ì •ì˜: {definition}")
            else:
                print(f"    ì •ì˜: (ì‚¬ì „ì— ë¯¸ë“±ë¡ëœ ìš©ì–´)")
            print()
    
    def batch_analyze(self, sentences: List[str], confidence_threshold: float = 0.3):
        for i, sentence in enumerate(sentences, 1):
            print(f"\n\n{'#'*70}")
            print(f"# ë¬¸ì¥ {i}/{len(sentences)}")
            print(f"{'#'*70}")
            self.analyze(sentence, confidence_threshold)


def main():
    MODEL_PATH = Path("models/final_model")
    TERM_DICT_PATH = Path("dataset/ìš©ì–´.json")
    analyzer = GameTermAnalyzer(MODEL_PATH, TERM_DICT_PATH)
    
    print("\nê²Œì„ ìš©ì–´ê°€ í¬í•¨ëœ ë¬¸ì¥ì„ ì…ë ¥í•˜ì„¸ìš”")
    print("ì¢…ë£Œí•˜ë ¤ë©´ që¥¼ ì…ë ¥í•˜ì„¸ìš”")
    
    while True:
        try:
            user_input = input("\n ë¬¸ì¥ ì…ë ¥: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'ì¢…ë£Œ', 'q']:
                print("\n í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            if not user_input:
                print("  ë¬¸ì¥ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                continue
            
            analyzer.analyze(user_input)
            
        except KeyboardInterrupt:
            print("\n\n í”„ë¡œê·¸ë¨ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"\n ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == "__main__":
    main()